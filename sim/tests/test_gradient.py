"""Tests for EqProp gradient computation.

Compares the EqProp symmetric-nudge gradient against numerical
finite-difference gradient for each weight.
"""

import numpy as np
import pytest

from eqprop.network import Network, solve_network
from eqprop.training import eqprop_gradient
from eqprop.xor import make_xor_network, make_inputs


@pytest.fixture
def net():
    return make_xor_network()


@pytest.fixture
def init_weights(net):
    """Same random initialization as training (seed=42)."""
    wp = net.weight_params
    rng = np.random.RandomState(42)
    G_init = rng.uniform(wp.G_min, wp.G_max, net.n_weights)
    return 1.0 / G_init


def _numerical_gradient(net, inputs, weights, target, eps=1e-5):
    """Finite-difference gradient dC/dG for each weight."""
    num_grad = np.zeros(net.n_weights)
    for w_idx in range(net.n_weights):
        G = 1.0 / weights[w_idx]
        for sign_val, label in [(+1, 'plus'), (-1, 'minus')]:
            w_test = weights.copy()
            w_test[w_idx] = 1.0 / (G + sign_val * eps)
            V_test = solve_network(net, inputs, w_test)
            pred_test = net.prediction(V_test)
            C_test = 0.5 * (target - pred_test) ** 2
            if sign_val == +1:
                C_plus = C_test
            else:
                C_minus = C_test
        num_grad[w_idx] = (C_plus - C_minus) / (2 * eps)
    return num_grad


# Skip pattern (4,1) which has known solver sensitivity with seed=42
@pytest.mark.parametrize("v_x1,v_x2,target", [
    (1.0, 1.0, 0.0),   # (0,0)
    (1.0, 4.0, 0.3),   # (0,1)
    (4.0, 4.0, 0.0),   # (1,1)
])
def test_eqprop_vs_finite_difference(net, init_weights, v_x1, v_x2, target):
    """EqProp gradient should match numerical gradient within 50% per weight."""
    inputs = make_inputs(v_x1, v_x2)
    beta = 1e-5

    eqprop_grad, _, _ = eqprop_gradient(
        net, inputs, init_weights, target, beta
    )
    num_grad = _numerical_gradient(net, inputs, init_weights, target)

    for w_idx in range(net.n_weights):
        eq = eqprop_grad[w_idx]
        nm = num_grad[w_idx]

        # Both near zero â€” skip
        if abs(eq) < 1e-10 and abs(nm) < 1e-10:
            continue

        # Relative error check (50% tolerance, same as original code)
        if abs(nm) > 1e-10:
            rel_err = abs(eq - nm) / abs(nm)
            assert rel_err < 0.5, (
                f"W{w_idx+1} pattern ({v_x1},{v_x2}): "
                f"EqProp={eq:+.6f} Numerical={nm:+.6f} rel_err={rel_err:.2f}"
            )
